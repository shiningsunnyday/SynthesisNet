{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "hash_dir = '/ssd/msun415/program_cache-bb=1000-prods=2/'\n",
    "programs = pickle.load(open(os.path.join(hash_dir, '2.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from synnet.utils.data_utils import binary_tree_to_skeleton, Program\n",
    "\n",
    "def prog_to_binary_tree(prog):\n",
    "    g = nx.DiGraph()\n",
    "    i = 0\n",
    "    child_dic = {}\n",
    "    for e in prog.entries:\n",
    "        if isinstance(e, tuple):\n",
    "            g.add_node(i)\n",
    "            child_dic[e[0]] = child_dic.get(e[0], []) + [(i, e[1]==0)]\n",
    "            i += 1\n",
    "        else:\n",
    "            g.add_node(i)\n",
    "            g.add_node(i+1)            \n",
    "            child_dic[e] = child_dic.get(e, []) + [(i, False), (i+1, True)]\n",
    "            i += 2\n",
    "    r_dic = {}    \n",
    "    for r in prog.rxn_tree:\n",
    "        n = len(g)\n",
    "        g.add_node(n)\n",
    "        for c in list(prog.rxn_tree[r]):\n",
    "            interm = r_dic[c]\n",
    "            left = prog.rxn_tree.nodes[c]['child'] == 'left'\n",
    "            g.add_edge(n, interm, left=left)\n",
    "            g.nodes[interm]['left'] = left\n",
    "        childs = child_dic[r]\n",
    "        for c, left in childs:    \n",
    "            g.add_edge(n, c, left=left)\n",
    "            g.nodes[c]['left'] = left\n",
    "        r_dic[r] = n\n",
    "    return g\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from tqdm import tqdm\n",
    "from tdc import Oracle\n",
    "\n",
    "max_size = 128\n",
    "batch_size = 10000\n",
    "\n",
    "heap = []\n",
    "\n",
    "\n",
    "# def scoring(prog):\n",
    "#     oracle = Oracle(name=\"QED\")     \n",
    "#     prog.product_map.load()\n",
    "#     score = 0.\n",
    "#     prod_map = prog.product_map._product_map\n",
    "#     e = 1 if d == 2 else 0\n",
    "#     scores = [oracle(prod) for prod in list(prod_map[e])]\n",
    "#     score = max(scores)\n",
    "#     prog.product_map.unload()\n",
    "#     print(score)\n",
    "#     return score\n",
    "\n",
    "\n",
    "def score_single(smi, d, i):\n",
    "    oracle = Oracle(name=\"QED\")\n",
    "    res = []\n",
    "    if isinstance(smi, list):\n",
    "        for s in smi:\n",
    "            score = oracle(smi)\n",
    "            res.append((score, d, i))\n",
    "    else:\n",
    "        score = oracle(smi)\n",
    "        return (score, d, i)\n",
    "    return res\n",
    "\n",
    "def score_list(lis):\n",
    "    assert isinstance(lis, list)\n",
    "    oracle = Oracle(name=\"QED\")\n",
    "    res = []\n",
    "    for smi, d, i in lis:\n",
    "        score = oracle(smi)\n",
    "        res.append((score, d, i))\n",
    "    return res\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "batches_per_program = 1\n",
    "fpath = 'SynthesisNet/scores-qed.txt'\n",
    "heap = []\n",
    "max_size = 128\n",
    "\n",
    "def load_file(fpath):\n",
    "    res = []\n",
    "    with open(fpath) as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in tqdm(lines, desc=\"processing lines\"):\n",
    "            line = line.strip('(').rstrip(')\\n')\n",
    "            if not line:\n",
    "                continue  \n",
    "            res.append(line)\n",
    "        return res\n",
    "    \n",
    "# lines = load_file(fpath)\n",
    "\n",
    "def re_eval():\n",
    "    for line in lines:\n",
    "        score, d, ind = line.split(', ')\n",
    "        print(line)\n",
    "        d = int(d)\n",
    "        i = int(ind)\n",
    "        prog = programs[d][int(ind)]\n",
    "        prog.product_map.load()\n",
    "        prod_map = prog.product_map._product_map\n",
    "        e = 1 if d == 2 else 0\n",
    "        smis = [(prod, d, i) for prod in list(prod_map[e])]\n",
    "        random.shuffle(smis)\n",
    "        smis = smis[:batches_per_program*batch_size] # only do first batch\n",
    "        inner_batch_size = (len(smis)+99)//100\n",
    "        smi_lists = [smis[i*inner_batch_size:(i+1)*inner_batch_size] for i in range(100)]\n",
    "        with mp.Pool(100) as p:\n",
    "            scores = p.map(score_list, tqdm(smi_lists, desc=\"single batch\"))        \n",
    "        res = [r for res_lis in scores for r in res_lis]\n",
    "        for (score, d, j), smi in zip(res, smis):\n",
    "            if len(heap) < max_size:\n",
    "                heapq.heappush(heap, (score, d, j, smi))\n",
    "            else:\n",
    "                small = heapq.heappushpop(heap, (score, d, j, smi))\n",
    "        print(heap)\n",
    "\n",
    "\n",
    "fpath = 'SynthesisNet/scores-logp.txt'\n",
    "def load_file2(fpath):\n",
    "    res = []\n",
    "    with open(fpath) as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in tqdm(lines, desc=\"processing lines\"):\n",
    "            line = line.strip('(').rstrip(')\\n')\n",
    "            if not line:\n",
    "                continue  \n",
    "            res.append(line)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synnet.encoding.fingerprints import fp_2048\n",
    "import json\n",
    "\n",
    "def lines_to_json(heap):\n",
    "    dics = []\n",
    "    for score, d, j, smi in heap:\n",
    "        dic = {}\n",
    "        d = int(d)\n",
    "        j = int(j)\n",
    "        tree = prog_to_binary_tree(programs[d][j])\n",
    "        root = next(v for v, d in tree.in_degree() if d == 0)\n",
    "        dic['bt'] = nx.tree_data(tree, root)\n",
    "        dic['fp'] = fp_2048(smi)\n",
    "        dic['smi'] = smi\n",
    "        dics.append(dic)\n",
    "    return dics\n",
    "\n",
    "# json.dump(lines_to_json(heap), open('SynthesisNet/indvs-qed.json', 'w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prop in ['drd2','jnk','logp']:\n",
    "    heap = [l.split(', ') for l in load_file2(f'SynthesisNet/scores-{prop}.txt')]\n",
    "    json.dump(lines_to_json(heap), open(f'SynthesisNet/indvs-{prop}.json', 'w+'))\n",
    "    print(f'SynthesisNet/indvs-{prop}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "base = 'SynthesisNet/results/logs/gnn'\n",
    "for version in os.listdir(base):\n",
    "    ckpts = Path(os.path.join(base, version)).rglob(\"*.ckpt\")\n",
    "    ckpts = list(ckpts)\n",
    "    if ckpts:\n",
    "        print(\"pass\")\n",
    "    else:\n",
    "        print(version)\n",
    "        shutil.rmtree(os.path.join(base, version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synnet.config import DELIM\n",
    "lines = open('SynthesisNet/output_reconstruct_top_k=3_max_num_rxns=3_max_rxns=-1.txt').readlines()\n",
    "recovered = {'targets': [], 'decoded': []}\n",
    "unrecovered = {'targets': [], 'decoded': []}\n",
    "for line in lines:\n",
    "    index, res, score = line.split(' ')\n",
    "    target, decoded, index = res.split(DELIM)\n",
    "    score = float(score)\n",
    "    if score == 1.0:\n",
    "        recovered['targets'].append(target)\n",
    "        recovered['decoded'].append(decoded)\n",
    "    else:\n",
    "        unrecovered['targets'].append(target)\n",
    "        unrecovered['decoded'].append(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc import Evaluator\n",
    "\n",
    "for metric in \"KL_divergence FCD_Distance Novelty Validity Uniqueness\".split():\n",
    "    evaluator = Evaluator(name=metric)\n",
    "    try:\n",
    "        score_recovered = evaluator(recovered[\"targets\"], recovered[\"decoded\"])\n",
    "        score_unrecovered = evaluator(unrecovered[\"targets\"], unrecovered[\"decoded\"])\n",
    "    except TypeError:\n",
    "        # Some evaluators only take 1 input args, try that.\n",
    "        score_recovered = evaluator(recovered[\"decoded\"])\n",
    "        score_unrecovered = evaluator(unrecovered[\"decoded\"])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        score_recovered, score_unrecovered = np.nan, np.nan\n",
    "\n",
    "    print(f\"Evaluation metric for {evaluator.name}:\")\n",
    "    print(f\"    Recovered score: {score_recovered:.2f}\")\n",
    "    print(f\"  Unrecovered score: {score_unrecovered:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unrecovered['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "\n",
    "for pat in ['Top 1','Top 10','Top 100','Top 1 SA','Top 10 SA','Top 100 SA','Top 1 AUC','Top 10 AUC','Top 100 AUC']:\n",
    "    df = pd.read_csv('SynthesisNet/data/pmo/results.csv', index_col=0)\n",
    "    heat_plot = {}\n",
    "    for col in df.columns:    \n",
    "        ranks = []        \n",
    "        if col[-len(pat):] != pat:\n",
    "            continue\n",
    "        if 'qed' in col:\n",
    "            continue\n",
    "        if '7l11' in col:\n",
    "            continue\n",
    "        if 'drd3' in col:\n",
    "            continue\n",
    "        for v in df[col]:\n",
    "            v = str(v)\n",
    "            if v == 'nan':\n",
    "                rank = -1\n",
    "            else:\n",
    "                start = v.index('(')\n",
    "                end = v.index('|')\n",
    "                rank = int(v[start+1:end])\n",
    "            ranks.append(rank)\n",
    "        heat_plot[col] = ranks\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for easier manipulation\n",
    "    df = pd.DataFrame(heat_plot, index=df.index)\n",
    "\n",
    "    # Create a custom color map\n",
    "    colors = [(0, \"green\"), (0.5, \"yellow\"), (1, \"red\")]  # From green (best) to red (worst)\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\"rank_cmap\", colors)\n",
    "\n",
    "    # Normalize the values for the color map (excluding -1)\n",
    "    norm = mcolors.Normalize(vmin=1, vmax=df.max().max())\n",
    "\n",
    "    # Set up the figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Create the heatmap\n",
    "    sns.heatmap(df.T, fmt=\"d\", cmap=cmap, cbar=False, linewidths=.5, \n",
    "                linecolor='black', mask=(df.T == -1), ax=ax, norm=norm)\n",
    "\n",
    "    # Manually add the black cells for -1\n",
    "    for i in range(df.shape[1]):\n",
    "        for j in range(df.shape[0]):\n",
    "            if df.iloc[j, i] == -1:\n",
    "                ax.add_patch(plt.Rectangle((i, j), 1, 1, fill=True, color='black', edgecolor='black'))\n",
    "\n",
    "    # Create the colorbar on the same axis\n",
    "    cbar = plt.colorbar(mappable=plt.cm.ScalarMappable(norm=norm, cmap=cmap), ax=ax, ticks=range(1, df.max().max() + 1))\n",
    "    cbar.ax.invert_yaxis()  # Invert the color bar so that green (best) is at the top\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xlabel('Baselines')\n",
    "    ax.set_ylabel('Metrics')\n",
    "    ax.set_title('Baseline Rankings Heatmap')\n",
    "\n",
    "    fig.savefig(f'SynthesisNet/data/pmo/rankings/{pat}.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
