{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:20:41 rdkit INFO: Enabling RDKit 2022.09.5 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "hash_dir = '/ssd/msun415/program_cache-bb=1000-prods=2/'\n",
    "programs = pickle.load(open(os.path.join(hash_dir, '2.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from synnet.utils.data_utils import binary_tree_to_skeleton, Program\n",
    "\n",
    "def prog_to_binary_tree(prog):\n",
    "    g = nx.DiGraph()\n",
    "    i = 0\n",
    "    child_dic = {}\n",
    "    for e in prog.entries:\n",
    "        if isinstance(e, tuple):\n",
    "            g.add_node(i)\n",
    "            child_dic[e[0]] = child_dic.get(e[0], []) + [(i, e[1]==0)]\n",
    "            i += 1\n",
    "        else:\n",
    "            g.add_node(i)\n",
    "            g.add_node(i+1)            \n",
    "            child_dic[e] = child_dic.get(e, []) + [(i, False), (i+1, True)]\n",
    "            i += 2\n",
    "    r_dic = {}    \n",
    "    for r in prog.rxn_tree:\n",
    "        n = len(g)\n",
    "        g.add_node(n)\n",
    "        for c in list(prog.rxn_tree[r]):\n",
    "            interm = r_dic[c]\n",
    "            left = prog.rxn_tree.nodes[c]['child'] == 'left'\n",
    "            g.add_edge(n, interm, left=left)\n",
    "            g.nodes[interm]['left'] = left\n",
    "        childs = child_dic[r]\n",
    "        for c, left in childs:    \n",
    "            g.add_edge(n, c, left=left)\n",
    "            g.nodes[c]['left'] = left\n",
    "        r_dic[r] = n\n",
    "    return g\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from tqdm import tqdm\n",
    "from tdc import Oracle\n",
    "\n",
    "max_size = 128\n",
    "batch_size = 10000\n",
    "\n",
    "heap = []\n",
    "\n",
    "\n",
    "# def scoring(prog):\n",
    "#     oracle = Oracle(name=\"QED\")     \n",
    "#     prog.product_map.load()\n",
    "#     score = 0.\n",
    "#     prod_map = prog.product_map._product_map\n",
    "#     e = 1 if d == 2 else 0\n",
    "#     scores = [oracle(prod) for prod in list(prod_map[e])]\n",
    "#     score = max(scores)\n",
    "#     prog.product_map.unload()\n",
    "#     print(score)\n",
    "#     return score\n",
    "\n",
    "\n",
    "def score_single(smi, d, i):\n",
    "    oracle = Oracle(name=\"QED\")\n",
    "    res = []\n",
    "    if isinstance(smi, list):\n",
    "        for s in smi:\n",
    "            score = oracle(smi)\n",
    "            res.append((score, d, i))\n",
    "    else:\n",
    "        score = oracle(smi)\n",
    "        return (score, d, i)\n",
    "    return res\n",
    "\n",
    "def score_list(lis):\n",
    "    assert isinstance(lis, list)\n",
    "    oracle = Oracle(name=\"QED\")\n",
    "    res = []\n",
    "    for smi, d, i in lis:\n",
    "        score = oracle(smi)\n",
    "        res.append((score, d, i))\n",
    "    return res\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "batches_per_program = 1\n",
    "fpath = '/home/msun415/SynTreeNet/scores-qed.txt'\n",
    "heap = []\n",
    "max_size = 128\n",
    "\n",
    "def load_file(fpath):\n",
    "    res = []\n",
    "    with open(fpath) as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in tqdm(lines, desc=\"processing lines\"):\n",
    "            line = line.strip('(').rstrip(')\\n')\n",
    "            if not line:\n",
    "                continue  \n",
    "            res.append(line)\n",
    "        return res\n",
    "    \n",
    "# lines = load_file(fpath)\n",
    "\n",
    "def re_eval():\n",
    "    for line in lines:\n",
    "        score, d, ind = line.split(', ')\n",
    "        print(line)\n",
    "        d = int(d)\n",
    "        i = int(ind)\n",
    "        prog = programs[d][int(ind)]\n",
    "        prog.product_map.load()\n",
    "        prod_map = prog.product_map._product_map\n",
    "        e = 1 if d == 2 else 0\n",
    "        smis = [(prod, d, i) for prod in list(prod_map[e])]\n",
    "        random.shuffle(smis)\n",
    "        smis = smis[:batches_per_program*batch_size] # only do first batch\n",
    "        inner_batch_size = (len(smis)+99)//100\n",
    "        smi_lists = [smis[i*inner_batch_size:(i+1)*inner_batch_size] for i in range(100)]\n",
    "        with mp.Pool(100) as p:\n",
    "            scores = p.map(score_list, tqdm(smi_lists, desc=\"single batch\"))        \n",
    "        res = [r for res_lis in scores for r in res_lis]\n",
    "        for (score, d, j), smi in zip(res, smis):\n",
    "            if len(heap) < max_size:\n",
    "                heapq.heappush(heap, (score, d, j, smi))\n",
    "            else:\n",
    "                small = heapq.heappushpop(heap, (score, d, j, smi))\n",
    "        print(heap)\n",
    "\n",
    "\n",
    "fpath = '/home/msun415/SynTreeNet/scores-logp.txt'\n",
    "def load_file2(fpath):\n",
    "    res = []\n",
    "    with open(fpath) as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in tqdm(lines, desc=\"processing lines\"):\n",
    "            line = line.strip('(').rstrip(')\\n')\n",
    "            if not line:\n",
    "                continue  \n",
    "            res.append(line)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synnet.encoding.fingerprints import fp_2048\n",
    "import json\n",
    "\n",
    "def lines_to_json(heap):\n",
    "    dics = []\n",
    "    for score, d, j, smi in heap:\n",
    "        dic = {}\n",
    "        d = int(d)\n",
    "        j = int(j)\n",
    "        tree = prog_to_binary_tree(programs[d][j])\n",
    "        root = next(v for v, d in tree.in_degree() if d == 0)\n",
    "        dic['bt'] = nx.tree_data(tree, root)\n",
    "        dic['fp'] = fp_2048(smi)\n",
    "        dic['smi'] = smi\n",
    "        dics.append(dic)\n",
    "    return dics\n",
    "\n",
    "# json.dump(lines_to_json(heap), open('/home/msun415/SynTreeNet/indvs-qed.json', 'w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing lines: 100%|██████████| 131/131 [00:00<00:00, 906689.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msun415/SynTreeNet/indvs-drd2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing lines: 100%|██████████| 131/131 [00:00<00:00, 1445931.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msun415/SynTreeNet/indvs-jnk.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing lines: 100%|██████████| 131/131 [00:00<00:00, 1489034.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msun415/SynTreeNet/indvs-logp.json\n"
     ]
    }
   ],
   "source": [
    "for prop in ['drd2','jnk','logp']:\n",
    "    heap = [l.split(', ') for l in load_file2(f'/home/msun415/SynTreeNet/scores-{prop}.txt')]\n",
    "    json.dump(lines_to_json(heap), open(f'/home/msun415/SynTreeNet/indvs-{prop}.json', 'w+'))\n",
    "    print(f'/home/msun415/SynTreeNet/indvs-{prop}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version_90\n",
      "version_8\n",
      "version_213\n",
      "version_109\n",
      "version_115\n",
      "pass\n",
      "version_103\n",
      "version_52\n",
      "version_91\n",
      "pass\n",
      "pass\n",
      "version_60\n",
      "pass\n",
      "version_70\n",
      "version_112\n",
      "version_6\n",
      "pass\n",
      "pass\n",
      "version_110\n",
      "pass\n",
      "version_49\n",
      "pass\n",
      "version_76\n",
      "pass\n",
      "version_82\n",
      "version_33\n",
      "version_102\n",
      "pass\n",
      "version_210\n",
      "version_94\n",
      "version_207\n",
      "version_2\n",
      "version_100\n",
      "version_47\n",
      "version_51\n",
      "version_87\n",
      "version_75\n",
      "version_5\n",
      "version_13\n",
      "version_25\n",
      "version_72\n",
      "pass\n",
      "pass\n",
      "version_48\n",
      "version_69\n",
      "version_106\n",
      "pass\n",
      "version_79\n",
      "version_92\n",
      "version_63\n",
      "version_208\n",
      "pass\n",
      "version_95\n",
      "version_211\n",
      "version_219\n",
      "version_39\n",
      "version_116\n",
      "version_9\n",
      "version_0\n",
      "version_59\n",
      "version_10\n",
      "version_28\n",
      "version_65\n",
      "pass\n",
      "version_117\n",
      "version_209\n",
      "pass\n",
      "version_64\n",
      "pass\n",
      "pass\n",
      "version_34\n",
      "pass\n",
      "version_50\n",
      "pass\n",
      "version_27\n",
      "version_66\n",
      "pass\n",
      "version_35\n",
      "version_78\n",
      "version_81\n",
      "version_41\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "pass\n",
      "version_71\n",
      "version_104\n",
      "pass\n",
      "pass\n",
      "version_29\n",
      "version_55\n",
      "version_7\n",
      "version_212\n",
      "pass\n",
      "pass\n",
      "version_222\n",
      "version_12\n",
      "version_77\n",
      "version_30\n",
      "pass\n",
      "version_220\n",
      "version_36\n",
      "gnn\n",
      "version_107\n",
      "version_73\n",
      "version_11\n",
      "version_43\n",
      "version_37\n",
      "pass\n",
      "version_15\n",
      "version_58\n",
      "version_98\n",
      "pass\n",
      "version_89\n",
      "version_40\n",
      "pass\n",
      "version_56\n",
      "version_1\n",
      "version_86\n",
      "version_32\n",
      "version_3\n",
      "pass\n",
      "version_105\n",
      "version_80\n",
      "version_111\n",
      "version_88\n",
      "version_24\n",
      "version_61\n",
      "version_93\n",
      "pass\n",
      "version_221\n",
      "version_68\n",
      "pass\n",
      "version_85\n",
      "version_4\n",
      "version_38\n",
      "pass\n",
      "version_67\n",
      "pass\n",
      "version_31\n",
      "pass\n",
      "pass\n",
      "version_118\n",
      "version_18\n",
      "version_214\n",
      "version_108\n",
      "version_84\n",
      "version_53\n",
      "version_62\n",
      "version_74\n",
      "pass\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "base = '/home/msun415/SynTreeNet/results/logs/gnn'\n",
    "for version in os.listdir(base):\n",
    "    ckpts = Path(os.path.join(base, version)).rglob(\"*.ckpt\")\n",
    "    ckpts = list(ckpts)\n",
    "    if ckpts:\n",
    "        print(\"pass\")\n",
    "    else:\n",
    "        print(version)\n",
    "        shutil.rmtree(os.path.join(base, version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synnet.config import DELIM\n",
    "lines = open('/home/msun415/SynTreeNet/output_reconstruct_top_k=3_max_num_rxns=3_max_rxns=-1.txt').readlines()\n",
    "recovered = {'targets': [], 'decoded': []}\n",
    "unrecovered = {'targets': [], 'decoded': []}\n",
    "for line in lines:\n",
    "    index, res, score = line.split(' ')\n",
    "    target, decoded, index = res.split(DELIM)\n",
    "    score = float(score)\n",
    "    if score == 1.0:\n",
    "        recovered['targets'].append(target)\n",
    "        recovered['decoded'].append(decoded)\n",
    "    else:\n",
    "        unrecovered['targets'].append(target)\n",
    "        unrecovered['decoded'].append(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metric for kl_divergence:\n",
      "    Recovered score: 1.00\n",
      "  Unrecovered score: 0.90\n",
      "Please install fcd by 'pip install FCD' (for Tensorflow backend)                                             or 'pip install fcd_torch' (for PyTorch backend)!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/synnet/lib/python3.9/site-packages/tdc/chem_utils/evaluator.py:435\u001b[0m, in \u001b[0;36mfcd_distance\u001b[0;34m(generated_smiles_lst, training_smiles_lst)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 435\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mfcd\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m fcd\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/synnet/lib/python3.9/site-packages/tdc/chem_utils/evaluator.py:440\u001b[0m, in \u001b[0;36mfcd_distance\u001b[0;34m(generated_smiles_lst, training_smiles_lst)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mfcd_torch\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fcd_distance_torch(generated_smiles_lst, training_smiles_lst)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fcd_torch'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     score_recovered \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecovered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecovered\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdecoded\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     score_unrecovered \u001b[38;5;241m=\u001b[39m evaluator(unrecovered[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m], unrecovered[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoded\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/synnet/lib/python3.9/site-packages/tdc/evaluator.py:470\u001b[0m, in \u001b[0;36mEvaluator.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01min\u001b[39;00m distribution_oracles:\n\u001b[0;32m--> 470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluator_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# \t#### evaluator for distribution learning, e.g., diversity, validity\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/synnet/lib/python3.9/site-packages/tdc/chem_utils/evaluator.py:444\u001b[0m, in \u001b[0;36mfcd_distance\u001b[0;34m(generated_smiles_lst, training_smiles_lst)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    445\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install fcd by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install FCD\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (for Tensorflow backend) \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;124m                                        or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install fcd_torch\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (for PyTorch backend)!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         )\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fcd_distance_tf(generated_smiles_lst, training_smiles_lst)\n",
      "\u001b[0;31mImportError\u001b[0m: Please install fcd by 'pip install FCD' (for Tensorflow backend)                                             or 'pip install fcd_torch' (for PyTorch backend)!",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(e)\n\u001b[0;32m---> 14\u001b[0m     score_recovered, score_unrecovered \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mnan, np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation metric for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Recovered score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore_recovered\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from tdc import Evaluator\n",
    "\n",
    "for metric in \"KL_divergence FCD_Distance Novelty Validity Uniqueness\".split():\n",
    "    evaluator = Evaluator(name=metric)\n",
    "    try:\n",
    "        score_recovered = evaluator(recovered[\"targets\"], recovered[\"decoded\"])\n",
    "        score_unrecovered = evaluator(unrecovered[\"targets\"], unrecovered[\"decoded\"])\n",
    "    except TypeError:\n",
    "        # Some evaluators only take 1 input args, try that.\n",
    "        score_recovered = evaluator(recovered[\"decoded\"])\n",
    "        score_unrecovered = evaluator(unrecovered[\"decoded\"])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        score_recovered, score_unrecovered = np.nan, np.nan\n",
    "\n",
    "    print(f\"Evaluation metric for {evaluator.name}:\")\n",
    "    print(f\"    Recovered score: {score_recovered:.2f}\")\n",
    "    print(f\"  Unrecovered score: {score_unrecovered:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4680"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unrecovered['targets'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synnet",
   "language": "python",
   "name": "synnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
